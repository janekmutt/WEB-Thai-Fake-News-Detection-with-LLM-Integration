import streamlit as st
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from gensim.models import KeyedVectors
from pythainlp.tokenize import word_tokenize
from pythainlp.tag import pos_tag
import re
import joblib  


st.title("ğŸ“° Fake News Detection in Thai")
st.markdown("""
Fake News Detection Model à¸—à¸µà¹ˆà¸à¸±à¸’à¸™à¸²à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ **LSTM Neural Network** 
à¹€à¸à¸·à¹ˆà¸­à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹à¸¥à¸°à¸ˆà¸³à¹à¸™à¸à¸‚à¹ˆà¸²à¸§à¸ªà¸²à¸£à¸­à¸­à¸à¹€à¸›à¹‡à¸™ **à¸‚à¹ˆà¸²à¸§à¸ˆà¸£à¸´à¸‡, à¸‚à¹ˆà¸²à¸§à¸›à¸¥à¸­à¸¡ à¹à¸¥à¸°à¸‚à¹ˆà¸²à¸§à¸šà¸´à¸”à¹€à¸šà¸·à¸­à¸™** à¹‚à¸”à¸¢à¸¡à¸µà¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³à¸à¸§à¹ˆà¸² **85%**  

ğŸ” à¹€à¸à¸µà¸¢à¸‡à¸›à¹‰à¸­à¸™à¸«à¸±à¸§à¸‚à¹‰à¸­à¸‚à¹ˆà¸²à¸§à¸¥à¸‡à¹„à¸› **à¸£à¸°à¸šà¸šà¸ˆà¸°à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹à¸¥à¸°à¹à¸ªà¸”à¸‡à¸œà¸¥à¸—à¸±à¸™à¸—à¸µ** à¸§à¹ˆà¸²à¸‚à¹ˆà¸²à¸§à¸—à¸µà¹ˆà¸„à¸¸à¸“à¸à¸³à¸¥à¸±à¸‡à¸­à¹ˆà¸²à¸™à¸­à¸¢à¸¹à¹ˆà¹€à¸›à¹‡à¸™à¸‚à¹ˆà¸²à¸§à¸ˆà¸£à¸´à¸‡à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ! à¸à¸£à¹‰à¸­à¸¡à¸—à¸±à¹‰à¸‡à¹à¸ªà¸”à¸‡à¸‚à¹ˆà¸²à¸§à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¹à¸¥à¸°à¸ªà¸£à¸¸à¸›à¹€à¸™à¸·à¹‰à¸­à¸«à¸²
""")

# ğŸ“Œ 1ï¸âƒ£ à¹‚à¸«à¸¥à¸” FastText Model
@st.cache_resource
def load_fasttext_model():
    return KeyedVectors.load_word2vec_format(r'C:\Users\cmanw\OneDrive\Documents\WEB-Thai-Fake-News-Detection-with-LLM-Integration\Model_Development\DL_model\gensim_fasttext_3000_vec300_e500_mc1.bin', binary=False)

fasttext_model = load_fasttext_model()

# ğŸ“Œ 2ï¸âƒ£ à¹‚à¸«à¸¥à¸” CountVectorizer à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¸•à¸­à¸™ Train
@st.cache_resource
def load_vectorizer():
    return joblib.load(r'C:\Users\cmanw\OneDrive\Documents\WEB-Thai-Fake-News-Detection-with-LLM-Integration\Model_Development\DL_model\vectorizer.pkl')

vectorizer = load_vectorizer()

# ğŸ“Œ 3ï¸âƒ£ à¹‚à¸«à¸¥à¸” Model à¸—à¸µà¹ˆ Train à¹„à¸§à¹‰ (à¹à¸à¹‰à¹ƒà¸«à¹‰à¸•à¸£à¸‡à¸à¸±à¸šà¸—à¸µà¹ˆà¸„à¸¸à¸“à¹ƒà¸Šà¹‰)
@st.cache_resource
def load_model():
    model_path = r"C:\Users\cmanw\OneDrive\Documents\WEB-Thai-Fake-News-Detection-with-LLM-Integration\Model_Development\DL_model\best_lstm_model_2025.h5"
    model = tf.keras.models.load_model(model_path, compile=False)
    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = load_model()

# ğŸ“Œ 4ï¸âƒ£ à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¹à¸›à¸¥à¸‡à¸„à¸³à¹€à¸›à¹‡à¸™à¹€à¸§à¸à¹€à¸•à¸­à¸£à¹Œà¸ˆà¸²à¸ FastText
def get_word2vec_embedding(text, model, vector_size=300):
    vec = np.zeros((vector_size,))
    count = 0
    for word in word_tokenize(text, engine="deepcut"):
        if word in model:
            vec += model[word]
            count += 1
    if count > 0:
        vec /= count  # à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¹€à¸§à¸à¹€à¸•à¸­à¸£à¹Œ
    return vec

# ğŸ“Œ 5ï¸âƒ£ à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™ Preprocess & Extract Features
def preprocess_and_extract_features(text):
    # ğŸ”¹ à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡
    cleaned_text = re.sub(r"[!\'\-#]", "", text)

    # ğŸ”¹ à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™à¹€à¸§à¸à¹€à¸•à¸­à¸£à¹Œ FastText
    word_vector = get_word2vec_embedding(cleaned_text, fasttext_model)

    # ğŸ”¹ à¸™à¸±à¸šà¸„à¸³à¸™à¸²à¸¡ (noun_count)
    tokens = word_tokenize(cleaned_text, engine="deepcut")
    pos_tags = pos_tag(tokens, engine="perceptron")
    noun_count = sum(1 for _, tag in pos_tags if tag.startswith("N"))

    # ğŸ”¹ à¸™à¸±à¸šà¸ˆà¸³à¸™à¸§à¸™à¸„à¸³ (Word_Count)
    word_count = len(tokens)

    # ğŸ”¹ NER (entity_count)
    entity_count = 0  

    # ğŸ”¹ à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ Bag of Words (sum_bow)
    bow_vector = vectorizer.transform([cleaned_text]).toarray()
    sum_bow = bow_vector.sum(axis=1)[0]

    # ğŸ”¹ à¸£à¸§à¸¡à¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
    numeric_features = np.array([noun_count, word_count, entity_count, sum_bow]).reshape(1, 1, 4)
    word_vector = word_vector.reshape(1, 1, -1)

    # ğŸ”¹ à¸£à¸§à¸¡ Word Embeddings + Features
    X_combined = np.concatenate([word_vector, numeric_features], axis=-1)

    return X_combined  # Shape (1, 1, 304)

# ğŸ“Œ 6ï¸âƒ£ à¸ªà¸£à¹‰à¸²à¸‡ Streamlit UI
# à¸£à¸±à¸šà¸­à¸´à¸™à¸à¸¸à¸•à¸ˆà¸²à¸à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰
user_input = st.text_area("âœï¸ à¸›à¹‰à¸­à¸™à¸«à¸±à¸§à¸‚à¹‰à¸­à¸‚à¹ˆà¸²à¸§", "")

if st.button("ğŸ” à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹ˆà¸²à¸§"):
    if user_input:
        # Preprocess à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡
        X_input = preprocess_and_extract_features(user_input)

        # à¸—à¸³ Prediction
        pred = model.predict(X_input)
        pred_class = np.argmax(pred)

        # à¹à¸ªà¸”à¸‡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ
        class_names = ["âœ… à¸‚à¹ˆà¸²à¸§à¸ˆà¸£à¸´à¸‡", "âš ï¸ à¸‚à¹ˆà¸²à¸§à¸›à¸¥à¸­à¸¡", "ğŸ”¶ à¸‚à¹ˆà¸²à¸§à¸šà¸´à¸”à¹€à¸šà¸·à¸­à¸™"]
        st.subheader(f"ğŸ“Œ à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ: {class_names[pred_class]}")
        
        # à¹à¸ªà¸”à¸‡ Probabilities
        st.write("ğŸ“Š à¸„à¹ˆà¸² Confidence à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸°à¸›à¸£à¸°à¹€à¸ à¸—:")
        st.json({class_names[i]: f"{prob*100:.2f}%" for i, prob in enumerate(pred[0])})
    else:
        st.warning("âš ï¸ à¸à¸£à¸¸à¸“à¸²à¸à¸£à¸­à¸à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸à¹ˆà¸­à¸™à¸à¸”à¸›à¸¸à¹ˆà¸¡à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š!")

# à¹€à¸„à¸£à¸”à¸´à¸•
st.markdown("---")
st.markdown("ğŸ”¹ Developed by CPE35 Student, KMUTT")
